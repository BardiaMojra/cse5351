Efficient Exascale Computing on Sierra Supercomputer
Sierra is one of the latest and fastest supercomputers in the world, currently second the nation and third internationally. Located at the Lawrence Livermore National Laboratory (LLNL), Sierra supercomputer was commissioned in 2018 to serve as the main high performace computing (HPC) system for the National Nuclear Security Administration (NNSA) Advanced Simulation and Computing (ASC) program. After the last underground test by the US, the program was created in 1995 in order to support the Stockpile Stewardship Program (SSP) and to ensure the safety and operational reliability of nation’s nuclear weapons stockpile. Sierra is used in junction with National Ignition Facility (NIF) to analyze and simulate nuclear weapon related experiments, e.g. nuclear explosions, storage in hostile or abnormal environments, and reentry simulations.
As a part of The Collaboration of Oak Ridge, Argonne, and Livermore (CORAL), Sierra is designed with state-of-the-art CPUs, GPUs, and high-frequency high-bandwith data interconnections and aims to deliver efficient exascale computing capabilities for discrete high-order finite element simulation and analysis [1]. Each node on Sierra is designed with IBM Power9 CPU with 22 cores and 4 threads per core. Each node hosts 4 NVIDIA V100 GPUs and in total, it has 4320 nodes which are interconnected by Mellonx Infiniband to support high-speed MPI computing. It is important to note that IBM’s Power9 CPU support NVLink, the fastest inter-GPU connection, which is also essential for efficient exascale computing. Sierra delivers theoretical peak performance of 125 PFLOPs and actual performance of 96 PFLOPs, per LINPACK benchmark [2].
Exascale computing will allow researchers to simulate large-scale high-order physical experiments in complex domains. This requires simulation source codes to utilize efficient data transfer executions in terms of both implementation and algorithmic complexity. To this end, the Department of Energy (DOE) Center for Efficient Exascale Discretizations (CEED) aims to bridge the gap by providing the CEED software stack for efficient exascale discretizations of high-order applications on GPU-accelerated platforms.
The idea behind efficient exascale computing architecture is the use of matrix-free high-order finite element discretizations, which require O(n) data movement and yield exponential convergence rates [1]. As described in [1], let Qp  represent the Lagrange polynomial bases on Gauss-Lobatto quadrature points which in the case of tensor products, the use of tensor-product sum-factorization reduces operator and memory/storage costs to near optimal rate, O(pd+1). Based on this realization, CEED software stack provides a purely algebraic intreface for matrix-free high-order finite element discretization on variety of CPUs and GPUs.
The CEED offers an array of libraries including MFEM, Nek5000/RS, libCEED, MAGMA, and libParanumal which are meant to enable developers implement efficiently parallelized high-order dynamical simulations. Figure 1 shows the block diagram for these libraries. These open-source libraries provide standard and expert-developed matrix-free linear algebra tools for modern and commercially available GPUs and CPUs. Moreover, the CEED has released a series bake-off problems (BP) to test and evaluate the performance of high-order codes. These include compute intensive kernels, k-neareast-neighbor communication, vector reduction and other problems that would be a fair representation of high-order problems.
MAGMA is a high-performance linear algebra that includes basic and general-purpose matrix compute kernels, e.g. BLAS, LAPACK for GPU. It is special designed for tensor operations on GPUs and previous work has shown its batched GEMM capabilities could be used to optimize non-tensor basis computations with the goal of hardware portability.
MFEM is a general purpose library for finite element simulation and it was developed to provide portable and state-of-the-art optimized performance high-order kernels for appropriate applications. MFEM architecture includes a flexible memory management unit with modular design for accelerated support with runtime-selectable backend interface for executing finite element kernels. MFEM’s GPU acceleration library has shown excellent performance on variety of single-GPU and multi-GPU benchmarks. Figure 2 (fig.6.) shows the performance of MFEM finite element for GPU acceleration on a machine with similar configuration to Sierra.
These libraries, experiments, and supercomputers, e.g. Sierra, allow researchers to investigate natural phenomena with evermore precision, accuracy, and speed. Sierra supercomputer, as well as other supercomputers at LLNL, build up the infrastructure that ensure nation’s nuclear deterrent capabilities while maintaining health and environmental safety by avoiding underground nuclear testing. Furthermore, these scientific tools and applications will be used in future to study climate change and forecast weather, predict composite matterial behavior in various environments, and endless large-scale finite element applications.
Corresponding Authors:
    I. tzanio@llnl.gov (T. Kolev).
    II. zwm-dcs@tsinghua.edu.cn (W. Zheng).
References:
    I. Abdelfattah, Ahmad and Barra, Valeria and Beams, Natalie and Bleile, Ryan and Brown, Jed and Camier, Jean-Sylvain and Carson, Robert and Chalmers, Noel and Dobrev, Veselin and Dudouit, Yohann and others, “GPU algorithms for efficient exascale discretizations,” Parallel Computing, 2021, PP. 102841, Elsevier.
    II. Zheng, “Research trend of large-scale supercomputers and applications from the TOP500 and Gordon Bell Prize,” Science China Information Sciences, 2020, PP. 1-14, Springer.
